2024-12-08 14:50:24 INFO     Saving logs in: ./logs/DeformingPlate/12_08/Grapher_14_50_24
2024-12-08 14:50:25 INFO     Loading DeformingPlate dataset
2024-12-08 16:35:31 INFO     Loading data costs  6306.60s
2024-12-08 16:35:31 INFO     Building models
2024-12-08 16:35:33 INFO     Model: Grapher(
  (in_mlp): MLP(
    (processor): Sequential(
      (0): Linear(in_features=4, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (blocks): ModuleList(
    (0-1): 2 x MultiscaleGraphBlock(
      (phy_aggr): PhysicsGraphBlock(
        (softmax): Softmax(dim=-1)
        (dropout): Dropout(p=0.0, inplace=False)
        (l_in): Linear(in_features=128, out_features=1024, bias=True)
        (l_token): Linear(in_features=128, out_features=1024, bias=True)
        (l_phy): Linear(in_features=128, out_features=32, bias=True)
        (q): Linear(in_features=128, out_features=128, bias=False)
        (k): Linear(in_features=128, out_features=128, bias=False)
        (v): Linear(in_features=128, out_features=128, bias=False)
        (l_out): Linear(in_features=1024, out_features=128, bias=True)
        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (ffn): MLP(
          (processor): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): Linear(in_features=128, out_features=128, bias=True)
            (3): ReLU()
            (4): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (global_aggr): AttentionGraphBlock(
        (graph_conv): GATv2Conv(128, 128, heads=8)
        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (ffn): MLP(
          (processor): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): ReLU()
            (5): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (local_aggr): AttentionGraphBlock(
        (graph_conv): GATv2Conv(128, 128, heads=8)
        (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (ffn): MLP(
          (processor): Sequential(
            (0): Linear(in_features=128, out_features=128, bias=True)
            (1): ReLU()
            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): ReLU()
            (5): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (processor): Sequential(
          (0): Linear(in_features=128, out_features=128, bias=True)
          (1): ReLU()
          (2): Linear(in_features=128, out_features=128, bias=True)
        )
      )
    )
  )
  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (out_mlp): MLP(
    (processor): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=1, bias=True)
    )
  )
)
2024-12-08 16:35:33 INFO     Criterion: <utils.loss.LpLoss object at 0x7fa8f45f3fd0>
2024-12-08 16:35:33 INFO     Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
2024-12-08 16:35:33 INFO     Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7fa8f45f3130>
2024-12-08 16:35:33 INFO     Building models costs  1.64s
2024-12-08 16:35:33 INFO     Start training
2024-12-08 16:35:33 INFO     Train dataset size: 60000
2024-12-08 16:35:33 INFO     Valid dataset size: 5000
2024-12-08 16:35:33 INFO     Test dataset size: 5000
2024-12-08 17:21:53 INFO     Epoch 0 | train_loss: 0.06267922 | Time: 2779.72s | lr: 0.0010
2024-12-08 17:24:42 INFO     Epoch 0 | valid_loss: 0.02616656 | Time: 169.36s
2024-12-08 17:24:43 INFO     Epoch 0 | save best models in ./logs/DeformingPlate/12_08/Grapher_14_50_24
2024-12-08 18:11:01 INFO     Epoch 1 | train_loss: 0.03539108 | Time: 2777.99s | lr: 0.0010
2024-12-08 18:13:50 INFO     Epoch 1 | valid_loss: 0.03217722 | Time: 169.58s
2024-12-08 19:00:03 INFO     Epoch 2 | train_loss: 0.03366317 | Time: 2772.40s | lr: 0.0010
2024-12-08 19:02:53 INFO     Epoch 2 | valid_loss: 0.02507091 | Time: 170.25s
2024-12-08 19:02:54 INFO     Epoch 2 | save best models in ./logs/DeformingPlate/12_08/Grapher_14_50_24
2024-12-08 19:49:05 INFO     Epoch 3 | train_loss: 0.03316072 | Time: 2771.27s | lr: 0.0010
2024-12-08 19:51:56 INFO     Epoch 3 | valid_loss: 0.03084618 | Time: 170.26s
2024-12-08 20:38:08 INFO     Epoch 4 | train_loss: 0.03137533 | Time: 2772.33s | lr: 0.0010
2024-12-08 20:40:58 INFO     Epoch 4 | valid_loss: 0.03836252 | Time: 169.77s
2024-12-08 21:27:14 INFO     Epoch 5 | train_loss: 0.02985117 | Time: 2775.85s | lr: 0.0010
2024-12-08 21:30:00 INFO     Epoch 5 | valid_loss: 0.02971049 | Time: 166.81s
2024-12-08 22:16:25 INFO     Epoch 6 | train_loss: 0.03115280 | Time: 2784.49s | lr: 0.0010
2024-12-08 22:19:16 INFO     Epoch 6 | valid_loss: 0.02888791 | Time: 170.71s
2024-12-08 23:05:30 INFO     Epoch 7 | train_loss: 0.02879615 | Time: 2774.55s | lr: 0.0010
2024-12-08 23:08:21 INFO     Epoch 7 | valid_loss: 0.03800601 | Time: 170.68s
2024-12-08 23:54:35 INFO     Epoch 8 | train_loss: 0.02882427 | Time: 2774.16s | lr: 0.0010
2024-12-08 23:57:23 INFO     Epoch 8 | valid_loss: 0.06573804 | Time: 168.21s
2024-12-09 00:43:36 INFO     Epoch 9 | train_loss: 0.02947928 | Time: 2773.14s | lr: 0.0010
2024-12-09 00:46:27 INFO     Epoch 9 | valid_loss: 0.04876776 | Time: 170.21s
2024-12-09 01:32:40 INFO     Epoch 10 | train_loss: 0.02924326 | Time: 2773.44s | lr: 0.0010
2024-12-09 01:35:29 INFO     Epoch 10 | valid_loss: 0.04338527 | Time: 169.32s
2024-12-09 02:21:52 INFO     Epoch 11 | train_loss: 0.02841572 | Time: 2782.66s | lr: 0.0010
2024-12-09 02:24:43 INFO     Epoch 11 | valid_loss: 0.03618146 | Time: 170.90s
2024-12-09 03:10:59 INFO     Epoch 12 | train_loss: 0.02806495 | Time: 2776.17s | lr: 0.0010
2024-12-09 03:13:47 INFO     Epoch 12 | valid_loss: 0.03046437 | Time: 167.59s
2024-12-09 04:00:01 INFO     Epoch 13 | train_loss: 0.02823393 | Time: 2774.11s | lr: 0.0010
2024-12-09 04:02:52 INFO     Epoch 13 | valid_loss: 0.03866936 | Time: 170.80s
2024-12-09 04:49:09 INFO     Epoch 14 | train_loss: 0.02869287 | Time: 2777.41s | lr: 0.0010
2024-12-09 04:52:00 INFO     Epoch 14 | valid_loss: 0.03932142 | Time: 170.52s
2024-12-09 05:38:17 INFO     Epoch 15 | train_loss: 0.02801027 | Time: 2777.27s | lr: 0.0010
2024-12-09 05:41:05 INFO     Epoch 15 | valid_loss: 0.02679291 | Time: 167.97s
2024-12-09 06:27:28 INFO     Epoch 16 | train_loss: 0.02802947 | Time: 2782.71s | lr: 0.0010
2024-12-09 06:30:18 INFO     Epoch 16 | valid_loss: 0.02843357 | Time: 170.33s
2024-12-09 07:16:29 INFO     Epoch 17 | train_loss: 0.02809257 | Time: 2771.03s | lr: 0.0010
2024-12-09 07:19:19 INFO     Epoch 17 | valid_loss: 0.03148355 | Time: 170.14s
2024-12-09 08:05:33 INFO     Epoch 18 | train_loss: 0.02887903 | Time: 2773.70s | lr: 0.0010
2024-12-09 08:08:24 INFO     Epoch 18 | valid_loss: 0.02650013 | Time: 170.67s
2024-12-09 08:54:39 INFO     Epoch 19 | train_loss: 0.02896490 | Time: 2775.97s | lr: 0.0010
2024-12-09 08:57:27 INFO     Epoch 19 | valid_loss: 0.02924888 | Time: 167.66s
2024-12-09 09:43:46 INFO     Epoch 20 | train_loss: 0.02746178 | Time: 2778.97s | lr: 0.0010
2024-12-09 09:46:33 INFO     Epoch 20 | valid_loss: 0.04419283 | Time: 167.25s
2024-12-09 10:32:54 INFO     Epoch 21 | train_loss: 0.02761588 | Time: 2780.76s | lr: 0.0010
2024-12-09 10:35:45 INFO     Epoch 21 | valid_loss: 0.03169934 | Time: 171.15s
2024-12-09 11:22:00 INFO     Epoch 22 | train_loss: 0.02757410 | Time: 2774.17s | lr: 0.0010
2024-12-09 11:24:50 INFO     Epoch 22 | valid_loss: 0.03902568 | Time: 170.10s
2024-12-09 12:11:08 INFO     Epoch 23 | train_loss: 0.02739667 | Time: 2778.41s | lr: 0.0010
2024-12-09 12:13:59 INFO     Epoch 23 | valid_loss: 0.02540441 | Time: 171.00s
