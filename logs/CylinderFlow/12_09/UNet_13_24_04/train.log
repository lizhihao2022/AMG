2024-12-09 13:24:04 INFO     Saving logs in: ./logs/CylinderFlow/12_09/UNet_13_24_04
2024-12-09 13:24:04 INFO     Loading CylinderFlow dataset
2024-12-09 13:48:13 INFO     Loading data costs  1448.52s
2024-12-09 13:48:13 INFO     Building models
2024-12-09 13:48:13 INFO     Model: UNet1d(
  (encoder1): Sequential(
    (enc1conv1): Conv1d(5, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc1norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc1tanh1): Tanh()
    (enc1conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc1norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc1tanh2): Tanh()
  )
  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder2): Sequential(
    (enc2conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc2norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc2tanh1): Tanh()
    (enc2conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc2norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc2tanh2): Tanh()
  )
  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder3): Sequential(
    (enc3conv1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc3norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc3tanh1): Tanh()
    (enc3conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc3norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc3tanh2): Tanh()
  )
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (encoder4): Sequential(
    (enc4conv1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc4norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc4tanh1): Tanh()
    (enc4conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (enc4norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (enc4tanh2): Tanh()
  )
  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (bottleneck): Sequential(
    (bottleneckconv1): Conv1d(1024, 2048, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (bottlenecknorm1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bottlenecktanh1): Tanh()
    (bottleneckconv2): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (bottlenecknorm2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (bottlenecktanh2): Tanh()
  )
  (upconv4): ConvTranspose1d(2048, 1024, kernel_size=(2,), stride=(2,))
  (decoder4): Sequential(
    (dec4conv1): Conv1d(2048, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec4norm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec4tanh1): Tanh()
    (dec4conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec4norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec4tanh2): Tanh()
  )
  (upconv3): ConvTranspose1d(1024, 512, kernel_size=(2,), stride=(2,))
  (decoder3): Sequential(
    (dec3conv1): Conv1d(1024, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec3norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec3tanh1): Tanh()
    (dec3conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec3norm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec3tanh2): Tanh()
  )
  (upconv2): ConvTranspose1d(512, 256, kernel_size=(2,), stride=(2,))
  (decoder2): Sequential(
    (dec2conv1): Conv1d(512, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec2norm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec2tanh1): Tanh()
    (dec2conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec2norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec2tanh2): Tanh()
  )
  (upconv1): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))
  (decoder1): Sequential(
    (dec1conv1): Conv1d(256, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec1norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec1tanh1): Tanh()
    (dec1conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (dec1norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dec1tanh2): Tanh()
  )
  (conv): Conv1d(128, 3, kernel_size=(1,), stride=(1,))
)
2024-12-09 13:48:13 INFO     Criterion: <utils.loss.MultipleLoss object at 0x7f767823fca0>
2024-12-09 13:48:13 INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0001
)
2024-12-09 13:48:13 INFO     Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7f767823ff70>
2024-12-09 13:48:13 INFO     Building models costs  0.28s
2024-12-09 13:48:13 INFO     Start training
2024-12-09 13:48:13 INFO     Train dataset size: 7600
2024-12-09 13:48:13 INFO     Valid dataset size: 1000
2024-12-09 13:48:13 INFO     Test dataset size: 1000
